{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Model**\n",
    "\n",
    "Train and validation the model using `wider_face_train` and `wider_face_val` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Define Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage.draw\n",
    "import json\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "from pycocotools.coco import COCO\n",
    "import pycocotools.mask as maskUtils\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath('D:/Hanifan/Face-Detection-MaskRCNN')\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import utils\n",
    "from mrcnn.utils import Dataset\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig(Config):\n",
    "    GPU_COUNT = 1\n",
    "    NAME = \"Face_Detection\"\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    IMAGES_PER_GPU = 1 \n",
    "    NUM_CLASSES = 1 + 1  # background + face\n",
    "    STEPS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def load_face(self, dataset_dir):\n",
    "        # Add classes\n",
    "        self.add_class(\"face\", 1, \"face\")\n",
    "        \n",
    "        annotations_dir = os.path.join(dataset_dir, 'annotations')\n",
    "        images_dir = os.path.join(dataset_dir, 'images')\n",
    "        \n",
    "        for json_file in os.listdir(annotations_dir):\n",
    "            if json_file.endswith(\".json\"):\n",
    "                json_path = os.path.join(annotations_dir, json_file)\n",
    "                \n",
    "                with open(json_path, 'r') as jsonfile:\n",
    "                    boundingboxes = json.load(jsonfile)\n",
    "                \n",
    "                image_filename = json_file[:-5] + \".jpg\"\n",
    "                \n",
    "                image_path = None\n",
    "                for subdir, _, files in os.walk(images_dir):\n",
    "                    if image_filename in files:\n",
    "                        image_path = os.path.join(subdir, image_filename)\n",
    "                        break\n",
    "                if not image_path:\n",
    "                    print(f\"Image file does not exist: {image_filename}\")\n",
    "                    continue\n",
    "                \n",
    "                image = skimage.io.imread(image_path)\n",
    "                if image.shape[0] > 1024:\n",
    "                    continue\n",
    "                \n",
    "                self.add_image(\n",
    "                    \"face\",\n",
    "                    image_id=image_filename,  # Use filename as a unique id\n",
    "                    path=image_path,\n",
    "                    width=image.shape[1],\n",
    "                    height=image.shape[0],\n",
    "                    boundingbox=boundingboxes\n",
    "                )\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"\n",
    "        Generate instance masks for shapes of given image ID\n",
    "        \"\"\"\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"face\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        \n",
    "        info = self.image_info[image_id]\n",
    "        boundingboxes = info['boundingbox']\n",
    "\n",
    "        # # Print bounding boxes for debugging\n",
    "        # print(f\"Bounding boxes for image {image_id}: {boundingboxes}\")\n",
    "\n",
    "        # Initialize mask\n",
    "        mask = np.zeros([info['height'], info['width'], len(boundingboxes)], dtype=np.uint8)\n",
    "        \n",
    "        for i, key in enumerate(boundingboxes.keys()):\n",
    "            box = boundingboxes[key]\n",
    "            if 'x' in box and 'y' in box:\n",
    "                x = np.clip(box['x'], 0, info['width'] - 1)\n",
    "                y = np.clip(box['y'], 0, info['height'] - 1)\n",
    "                rr, cc = skimage.draw.polygon(y, x)\n",
    "                mask[rr, cc, i] = 1\n",
    "            else:\n",
    "                print(f\"Invalid bounding box format for image {image_id}: {box}\")\n",
    "        \n",
    "        return mask, np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "    \n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"face\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            return super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train or Inference the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           Face_Detection\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  False\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Preparing training dataset...\n",
      "Preparing validation dataset...\n",
      "Starting training...\n",
      "\n",
      "Starting at epoch 0. LR=0.0001\n",
      "\n",
      "Checkpoint Path: D:\\Hanifan\\Face-Detection-MaskRCNN\\logs\\face_detection20240807T2327\\mask_rcnn_face_detection_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_2:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_5:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_8:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_11:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_2:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_5:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_8:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_11:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_1_grad/Reshape_1:0\", shape=(6000,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_1_grad/Reshape:0\", shape=(6000, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_1_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - batch: 49.5000 - size: 1.0000 - loss: 2.9028 - rpn_class_loss: 0.2087 - rpn_bbox_loss: 1.3977 - mrcnn_class_loss: 0.1698 - mrcnn_bbox_loss: 0.6472 - mrcnn_mask_loss: 0.4794"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\anis\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 162s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 2.9028 - rpn_class_loss: 0.2087 - rpn_bbox_loss: 1.3977 - mrcnn_class_loss: 0.1698 - mrcnn_bbox_loss: 0.6472 - mrcnn_mask_loss: 0.4794 - val_loss: 2.1130 - val_rpn_class_loss: 0.1063 - val_rpn_bbox_loss: 0.8590 - val_mrcnn_class_loss: 0.1882 - val_mrcnn_bbox_loss: 0.5953 - val_mrcnn_mask_loss: 0.3642\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 180s 2s/step - batch: 49.5000 - size: 1.0000 - loss: 2.0502 - rpn_class_loss: 0.1190 - rpn_bbox_loss: 0.8092 - mrcnn_class_loss: 0.2058 - mrcnn_bbox_loss: 0.5191 - mrcnn_mask_loss: 0.3971 - val_loss: 1.9885 - val_rpn_class_loss: 0.1656 - val_rpn_bbox_loss: 0.6792 - val_mrcnn_class_loss: 0.2056 - val_mrcnn_bbox_loss: 0.5268 - val_mrcnn_mask_loss: 0.4113\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 143s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.8438 - rpn_class_loss: 0.1232 - rpn_bbox_loss: 0.6373 - mrcnn_class_loss: 0.1883 - mrcnn_bbox_loss: 0.4942 - mrcnn_mask_loss: 0.4008 - val_loss: 2.1353 - val_rpn_class_loss: 0.1284 - val_rpn_bbox_loss: 0.9060 - val_mrcnn_class_loss: 0.1759 - val_mrcnn_bbox_loss: 0.5156 - val_mrcnn_mask_loss: 0.4093\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 117s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.7019 - rpn_class_loss: 0.0929 - rpn_bbox_loss: 0.5967 - mrcnn_class_loss: 0.1892 - mrcnn_bbox_loss: 0.4406 - mrcnn_mask_loss: 0.3824 - val_loss: 1.6492 - val_rpn_class_loss: 0.0886 - val_rpn_bbox_loss: 0.5276 - val_mrcnn_class_loss: 0.1902 - val_mrcnn_bbox_loss: 0.4759 - val_mrcnn_mask_loss: 0.3669\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 123s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.5038 - rpn_class_loss: 0.0861 - rpn_bbox_loss: 0.4697 - mrcnn_class_loss: 0.1878 - mrcnn_bbox_loss: 0.3940 - mrcnn_mask_loss: 0.3662 - val_loss: 1.8517 - val_rpn_class_loss: 0.1059 - val_rpn_bbox_loss: 0.7659 - val_mrcnn_class_loss: 0.1586 - val_mrcnn_bbox_loss: 0.4467 - val_mrcnn_mask_loss: 0.3747\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 128s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.5407 - rpn_class_loss: 0.0742 - rpn_bbox_loss: 0.4755 - mrcnn_class_loss: 0.2149 - mrcnn_bbox_loss: 0.4059 - mrcnn_mask_loss: 0.3702 - val_loss: 1.4990 - val_rpn_class_loss: 0.0769 - val_rpn_bbox_loss: 0.4735 - val_mrcnn_class_loss: 0.1697 - val_mrcnn_bbox_loss: 0.4114 - val_mrcnn_mask_loss: 0.3675\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 121s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.5997 - rpn_class_loss: 0.1273 - rpn_bbox_loss: 0.5085 - mrcnn_class_loss: 0.1983 - mrcnn_bbox_loss: 0.4031 - mrcnn_mask_loss: 0.3625 - val_loss: 1.4517 - val_rpn_class_loss: 0.0863 - val_rpn_bbox_loss: 0.4884 - val_mrcnn_class_loss: 0.1506 - val_mrcnn_bbox_loss: 0.3743 - val_mrcnn_mask_loss: 0.3521\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 139s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.4784 - rpn_class_loss: 0.0898 - rpn_bbox_loss: 0.4518 - mrcnn_class_loss: 0.1801 - mrcnn_bbox_loss: 0.3947 - mrcnn_mask_loss: 0.3620 - val_loss: 1.4972 - val_rpn_class_loss: 0.0878 - val_rpn_bbox_loss: 0.4808 - val_mrcnn_class_loss: 0.2061 - val_mrcnn_bbox_loss: 0.3702 - val_mrcnn_mask_loss: 0.3523\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 171s 2s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3888 - rpn_class_loss: 0.0686 - rpn_bbox_loss: 0.4432 - mrcnn_class_loss: 0.1513 - mrcnn_bbox_loss: 0.3734 - mrcnn_mask_loss: 0.3523 - val_loss: 1.5522 - val_rpn_class_loss: 0.1034 - val_rpn_bbox_loss: 0.5277 - val_mrcnn_class_loss: 0.2059 - val_mrcnn_bbox_loss: 0.3702 - val_mrcnn_mask_loss: 0.3450\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 117s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3506 - rpn_class_loss: 0.0558 - rpn_bbox_loss: 0.4419 - mrcnn_class_loss: 0.1490 - mrcnn_bbox_loss: 0.3559 - mrcnn_mask_loss: 0.3479 - val_loss: 1.3546 - val_rpn_class_loss: 0.0662 - val_rpn_bbox_loss: 0.4193 - val_mrcnn_class_loss: 0.1649 - val_mrcnn_bbox_loss: 0.3597 - val_mrcnn_mask_loss: 0.3445\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 145s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.4105 - rpn_class_loss: 0.0961 - rpn_bbox_loss: 0.4330 - mrcnn_class_loss: 0.1833 - mrcnn_bbox_loss: 0.3605 - mrcnn_mask_loss: 0.3376 - val_loss: 1.3522 - val_rpn_class_loss: 0.0760 - val_rpn_bbox_loss: 0.3719 - val_mrcnn_class_loss: 0.1749 - val_mrcnn_bbox_loss: 0.3747 - val_mrcnn_mask_loss: 0.3548\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 118s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3461 - rpn_class_loss: 0.0683 - rpn_bbox_loss: 0.4078 - mrcnn_class_loss: 0.1828 - mrcnn_bbox_loss: 0.3560 - mrcnn_mask_loss: 0.3313 - val_loss: 1.4616 - val_rpn_class_loss: 0.0759 - val_rpn_bbox_loss: 0.5042 - val_mrcnn_class_loss: 0.1557 - val_mrcnn_bbox_loss: 0.3832 - val_mrcnn_mask_loss: 0.3427\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 130s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3973 - rpn_class_loss: 0.0656 - rpn_bbox_loss: 0.4264 - mrcnn_class_loss: 0.1677 - mrcnn_bbox_loss: 0.3867 - mrcnn_mask_loss: 0.3510 - val_loss: 1.4862 - val_rpn_class_loss: 0.0958 - val_rpn_bbox_loss: 0.5283 - val_mrcnn_class_loss: 0.1712 - val_mrcnn_bbox_loss: 0.3706 - val_mrcnn_mask_loss: 0.3204\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 168s 2s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3853 - rpn_class_loss: 0.0693 - rpn_bbox_loss: 0.4373 - mrcnn_class_loss: 0.1659 - mrcnn_bbox_loss: 0.3720 - mrcnn_mask_loss: 0.3408 - val_loss: 1.3883 - val_rpn_class_loss: 0.0848 - val_rpn_bbox_loss: 0.4399 - val_mrcnn_class_loss: 0.1651 - val_mrcnn_bbox_loss: 0.3580 - val_mrcnn_mask_loss: 0.3406\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 146s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3639 - rpn_class_loss: 0.0859 - rpn_bbox_loss: 0.4162 - mrcnn_class_loss: 0.1984 - mrcnn_bbox_loss: 0.3396 - mrcnn_mask_loss: 0.3238 - val_loss: 1.4527 - val_rpn_class_loss: 0.0817 - val_rpn_bbox_loss: 0.4361 - val_mrcnn_class_loss: 0.2042 - val_mrcnn_bbox_loss: 0.3899 - val_mrcnn_mask_loss: 0.3408\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 135s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3398 - rpn_class_loss: 0.0742 - rpn_bbox_loss: 0.4389 - mrcnn_class_loss: 0.1569 - mrcnn_bbox_loss: 0.3380 - mrcnn_mask_loss: 0.3318 - val_loss: 1.4674 - val_rpn_class_loss: 0.0695 - val_rpn_bbox_loss: 0.4830 - val_mrcnn_class_loss: 0.1766 - val_mrcnn_bbox_loss: 0.3856 - val_mrcnn_mask_loss: 0.3527\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 138s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3603 - rpn_class_loss: 0.0726 - rpn_bbox_loss: 0.4405 - mrcnn_class_loss: 0.1814 - mrcnn_bbox_loss: 0.3446 - mrcnn_mask_loss: 0.3212 - val_loss: 1.4819 - val_rpn_class_loss: 0.1049 - val_rpn_bbox_loss: 0.5076 - val_mrcnn_class_loss: 0.1999 - val_mrcnn_bbox_loss: 0.3242 - val_mrcnn_mask_loss: 0.3454\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 126s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3516 - rpn_class_loss: 0.0658 - rpn_bbox_loss: 0.4333 - mrcnn_class_loss: 0.1750 - mrcnn_bbox_loss: 0.3435 - mrcnn_mask_loss: 0.3340 - val_loss: 1.2675 - val_rpn_class_loss: 0.0481 - val_rpn_bbox_loss: 0.3343 - val_mrcnn_class_loss: 0.1568 - val_mrcnn_bbox_loss: 0.3833 - val_mrcnn_mask_loss: 0.3449\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 139s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.2608 - rpn_class_loss: 0.0521 - rpn_bbox_loss: 0.3772 - mrcnn_class_loss: 0.1506 - mrcnn_bbox_loss: 0.3459 - mrcnn_mask_loss: 0.3349 - val_loss: 1.3477 - val_rpn_class_loss: 0.0631 - val_rpn_bbox_loss: 0.3984 - val_mrcnn_class_loss: 0.1743 - val_mrcnn_bbox_loss: 0.3674 - val_mrcnn_mask_loss: 0.3445\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 146s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3260 - rpn_class_loss: 0.0759 - rpn_bbox_loss: 0.4158 - mrcnn_class_loss: 0.1865 - mrcnn_bbox_loss: 0.3223 - mrcnn_mask_loss: 0.3255 - val_loss: 1.5569 - val_rpn_class_loss: 0.1293 - val_rpn_bbox_loss: 0.5733 - val_mrcnn_class_loss: 0.1835 - val_mrcnn_bbox_loss: 0.3410 - val_mrcnn_mask_loss: 0.3297\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 134s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.2502 - rpn_class_loss: 0.0614 - rpn_bbox_loss: 0.3759 - mrcnn_class_loss: 0.1661 - mrcnn_bbox_loss: 0.3251 - mrcnn_mask_loss: 0.3217 - val_loss: 1.2899 - val_rpn_class_loss: 0.0673 - val_rpn_bbox_loss: 0.3768 - val_mrcnn_class_loss: 0.1672 - val_mrcnn_bbox_loss: 0.3506 - val_mrcnn_mask_loss: 0.3280\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 144s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.2410 - rpn_class_loss: 0.0609 - rpn_bbox_loss: 0.3666 - mrcnn_class_loss: 0.1691 - mrcnn_bbox_loss: 0.3185 - mrcnn_mask_loss: 0.3259 - val_loss: 1.3329 - val_rpn_class_loss: 0.0893 - val_rpn_bbox_loss: 0.4122 - val_mrcnn_class_loss: 0.1548 - val_mrcnn_bbox_loss: 0.3477 - val_mrcnn_mask_loss: 0.3289\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 155s 2s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3292 - rpn_class_loss: 0.0687 - rpn_bbox_loss: 0.3890 - mrcnn_class_loss: 0.1894 - mrcnn_bbox_loss: 0.3469 - mrcnn_mask_loss: 0.3353 - val_loss: 1.4303 - val_rpn_class_loss: 0.0728 - val_rpn_bbox_loss: 0.4824 - val_mrcnn_class_loss: 0.1760 - val_mrcnn_bbox_loss: 0.3543 - val_mrcnn_mask_loss: 0.3448\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 159s 2s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3012 - rpn_class_loss: 0.0639 - rpn_bbox_loss: 0.3994 - mrcnn_class_loss: 0.1731 - mrcnn_bbox_loss: 0.3348 - mrcnn_mask_loss: 0.3299 - val_loss: 1.2961 - val_rpn_class_loss: 0.0869 - val_rpn_bbox_loss: 0.4241 - val_mrcnn_class_loss: 0.1798 - val_mrcnn_bbox_loss: 0.2973 - val_mrcnn_mask_loss: 0.3080\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 136s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.2490 - rpn_class_loss: 0.0728 - rpn_bbox_loss: 0.3632 - mrcnn_class_loss: 0.1829 - mrcnn_bbox_loss: 0.3110 - mrcnn_mask_loss: 0.3191 - val_loss: 1.2049 - val_rpn_class_loss: 0.0444 - val_rpn_bbox_loss: 0.3649 - val_mrcnn_class_loss: 0.1785 - val_mrcnn_bbox_loss: 0.3087 - val_mrcnn_mask_loss: 0.3084\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 120s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.2401 - rpn_class_loss: 0.0608 - rpn_bbox_loss: 0.3680 - mrcnn_class_loss: 0.1724 - mrcnn_bbox_loss: 0.3177 - mrcnn_mask_loss: 0.3211 - val_loss: 1.2559 - val_rpn_class_loss: 0.0613 - val_rpn_bbox_loss: 0.4118 - val_mrcnn_class_loss: 0.1590 - val_mrcnn_bbox_loss: 0.3152 - val_mrcnn_mask_loss: 0.3087\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 119s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.2813 - rpn_class_loss: 0.0841 - rpn_bbox_loss: 0.3932 - mrcnn_class_loss: 0.1603 - mrcnn_bbox_loss: 0.3249 - mrcnn_mask_loss: 0.3189 - val_loss: 1.3020 - val_rpn_class_loss: 0.0735 - val_rpn_bbox_loss: 0.3909 - val_mrcnn_class_loss: 0.1594 - val_mrcnn_bbox_loss: 0.3567 - val_mrcnn_mask_loss: 0.3214\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 154s 2s/step - batch: 49.5000 - size: 1.0000 - loss: 1.3419 - rpn_class_loss: 0.0828 - rpn_bbox_loss: 0.4325 - mrcnn_class_loss: 0.1769 - mrcnn_bbox_loss: 0.3261 - mrcnn_mask_loss: 0.3235 - val_loss: 1.3441 - val_rpn_class_loss: 0.0709 - val_rpn_bbox_loss: 0.4110 - val_mrcnn_class_loss: 0.1769 - val_mrcnn_bbox_loss: 0.3517 - val_mrcnn_mask_loss: 0.3336\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 137s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.1880 - rpn_class_loss: 0.0526 - rpn_bbox_loss: 0.3565 - mrcnn_class_loss: 0.1811 - mrcnn_bbox_loss: 0.2916 - mrcnn_mask_loss: 0.3061 - val_loss: 1.2139 - val_rpn_class_loss: 0.0523 - val_rpn_bbox_loss: 0.3474 - val_mrcnn_class_loss: 0.1502 - val_mrcnn_bbox_loss: 0.3356 - val_mrcnn_mask_loss: 0.3284\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 135s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 1.2726 - rpn_class_loss: 0.0762 - rpn_bbox_loss: 0.3830 - mrcnn_class_loss: 0.1819 - mrcnn_bbox_loss: 0.3143 - mrcnn_mask_loss: 0.3172 - val_loss: 1.2683 - val_rpn_class_loss: 0.0505 - val_rpn_bbox_loss: 0.4102 - val_mrcnn_class_loss: 0.1444 - val_mrcnn_bbox_loss: 0.3422 - val_mrcnn_mask_loss: 0.3210\n",
      "<tensorflow.python.keras.callbacks.History object at 0x0000022EB6758F10>\n"
     ]
    }
   ],
   "source": [
    "# Choose 'train' or 'inference'\n",
    "if __name__ == \"__main__\":\n",
    "    command = \"train\"\n",
    "\n",
    "    if command == \"train\":\n",
    "        config = DatasetConfig()\n",
    "    else:\n",
    "        class InferenceConfig(DatasetConfig):\n",
    "            GPU_COUNT = 1\n",
    "            IMAGES_PER_GPU = 1\n",
    "        config = InferenceConfig()\n",
    "    config.display()\n",
    "\n",
    "    if command == \"train\":\n",
    "        # Create model in training mode\n",
    "        model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "        \n",
    "        # Load pre-trained weights (optional, if available)\n",
    "        coco_weights_path = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "        if not os.path.exists(coco_weights_path):\n",
    "            utils.download_trained_weights(coco_weights_path)\n",
    "\n",
    "        # Load weights trained on MS-COCO\n",
    "        model.load_weights(coco_weights_path, by_name=True,\n",
    "                        exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "                                    \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "        # Training dataset\n",
    "        print(\"Preparing training dataset...\")\n",
    "        dataset_train = FaceDataset()\n",
    "        dataset_train.load_face(\"D:/Hanifan/Face-Detection-MaskRCNN/wider_face_split/wider_face_train\")\n",
    "        dataset_train.prepare()\n",
    "\n",
    "        # Validation dataset\n",
    "        print(\"Preparing validation dataset...\")\n",
    "        dataset_val = FaceDataset()\n",
    "        dataset_val.load_face(\"D:/Hanifan/Face-Detection-MaskRCNN/wider_face_split/wider_face_val\")\n",
    "        dataset_val.prepare()\n",
    "\n",
    "        # Proceed with training if all files are validated\n",
    "        print(\"Starting training...\")\n",
    "        history = model.train(dataset_train, dataset_val,\n",
    "                                learning_rate=config.LEARNING_RATE / 10,\n",
    "                                epochs=30,\n",
    "                                layers='heads')\n",
    "        print(history)\n",
    "    else:\n",
    "        model = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_face.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
